---
title: "Extracting S1 global"
params: 
  ExperimentID:
  rootdir: /home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing
output:
  html_document: 
    df_print: paged
    toc: yes
  html_notebook:
    code_folding: hide
    toc: yes
---



```{r setup, include= FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = F)
```

```{r, include= F}
library(tidyverse)
library(stringr)
```






#Read and Describe Data
```{r}
##Extracting N-back from 9 different countries --> merge dataset


nbackFR <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-1back-3back_France_2021-11-29.csv", sep = ",") #457708 obs

nbackIT <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-1back-3back_Italy_2021-11-29.csv", sep = ",") #324691 obs

nbackJP <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-1back-3back_Japan_2021-11-29.csv", sep = ",") #299274 obs

nbackTR <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-1back-3back_Turkey_2021-11-29.csv", sep = ",") #278363 obs

nbackGR <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-1back-3back_Greece_2021-11-29.csv", sep = ",") #225137 obs

nbackAR <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-1back-3back_Argentina_2021-11-29.csv", sep = ",") #172131 obs

nbackIN <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-1back-3back_India_2021-11-29.csv", sep = ",") %>% #61950 obs
  mutate(Local_Timezone = as.integer(parse_number(Local_Timezone)))

nbackCA <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-1back-3back_Canada_2021-11-29.csv", sep = ",") #30893 obs




Nback <- bind_rows(nbackFR, nbackIT, nbackJP, nbackTR, nbackAR, nbackGR, nbackIN, nbackCA)  %>%  #1.565.912 obs
  filter(str_detect(display, "block1back") | str_detect(display, "block3back")) %>%
  select(Session, Country, `Experiment_ID`, PID, Run, Unique_Name, ILI, duration, randomise_blocks,`Zone_Type`, duration, Response, `Spreadsheet_Name`, `Reaction_Time`, Correct, Attempt, `UTC_Date`, Handedness, Sex, Age)%>%
  rename(nback = Unique_Name,
         Spreadsheet = Spreadsheet_Name,
        randomblock = randomise_blocks) %>%
  mutate(ILI = as.factor(ifelse(ILI  == 1500, "1500ms", "1800ms")),
         duration = as.factor(ifelse(duration  == 45, "45s", "90s")),
         condition = ifelse(duration == "45s"  & ILI == "1500ms", 1, 0),
         condition = ifelse(duration == "90s" & ILI == "1500ms", 2, condition),
         condition = ifelse(duration == "45s"  & ILI == "1800ms", 3, condition),
         condition = ifelse(duration == "90s"  & ILI == "1800ms", 4, condition),
         Spreadsheet = ifelse(Spreadsheet == "n_back 1", "n_back1", Spreadsheet),
         Spreadsheet = ifelse(Spreadsheet == "n_back 2", "n_back2", Spreadsheet),
         Spreadsheet = ifelse(Spreadsheet == "n_back 3", "n_back3", Spreadsheet),
         timestamp = str_replace_all(UTC_Date, " ", ""),
         timestamp = str_replace_all(timestamp, "-", ""),
         timestamp = str_replace_all(timestamp, ":", ""),
         timestamp = as.numeric(timestamp))

```


#Remove duplicates + counting ID and observations by Country
```{r}
Nback %>%
  count(PID) #1174 participants

#list of participants to remove --> some participants logged twice or 3 x with a different login
#removing which participants ID that we don't want
participants_to_rm = c("1377295", "1373766", "1347792", "1358571","1348787","1318460",  "1347194", "1366463", "1330740", "1314652")
Nback<- Nback %>%
  filter(!(PID %in% participants_to_rm))

#nb of obs by Country (8 countries)
nb_obs <- Nback %>%
  group_by(Country) %>%
  summarise(observations = n())
nb_obs

#nb of subjects by Country (8 countries)  AR:128 (-17)/CA:33/FR:412 (-1)/GR:133/IN:53/IT:156 (+1)/JP:108/TR:146
nb_subjects <- Nback %>%
  group_by(PID) %>%
  summarise(Country = first(Country)) %>%
  group_by(Country) %>%
  summarise(numbersubjects = n())
nb_subjects

Nback %>%
  count(PID)   #1169 participants
```


#Read spreadsheets used for each trials and add variable that indicates the number of target letters
```{r}
#load spreadsheets
sprdsheet1b1 <-read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/data/spreadsheet1back_1.csv",  sep = ",") %>%
  mutate(Spreadsheet = "n_back1")
sprdsheet1b2 <-read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/data/spreadsheet1back_2.csv", sep = ",")%>%
  mutate(Spreadsheet = "n_back2")
sprdsheet1b3 <-read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/data/spreadsheet1back_3.csv", sep = ",")%>%
  mutate(Spreadsheet = "n_back3")
sprdsheet3b1 <-read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/data/spreadsheet3back_1.csv", sep = ",")%>%
  mutate(Spreadsheet = "n_back1")
sprdsheet3b2 <-read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/data/spreadsheet3back_2.csv", sep = ",")%>%
  mutate(Spreadsheet = "n_back2")
sprdsheet3b3 <-read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/data/spreadsheet3back_3.csv", sep = ",")%>%
  mutate(Spreadsheet = "n_back3")

#match spreadshets
sprdsheet <- bind_rows(sprdsheet1b1, sprdsheet1b2, sprdsheet3b1, sprdsheet3b2) %>%
   mutate(condition = ifelse(duration == "45s"  & ILI == 1500, 1, 0),
         condition = ifelse(duration == "90s" & ILI == 1500, 2, condition),
         condition = ifelse(duration == "45s"  & ILI == 1800, 3, condition),
         condition = ifelse(duration == "90s"  & ILI == 1800, 4, condition)) %>%
   filter(display == "block1back" | display == "block3back") %>%
        mutate(nback = ifelse(display == "block1back", "1back", "3back"))

 #determine nb of target and non-target letters for each set of conditions
nb_target <- sprdsheet %>%
  filter(condition != 0) %>%
  group_by(nback, Spreadsheet, condition) %>%
  summarise(ntarget = sum(ANSWER == "Left"), notarget = sum(ANSWER == "Down"))
nb_target

#verify value for colum spreadsheet
Nback %>%
  group_by(Spreadsheet) %>%
  summarise(n = n())

#nback1 becomes n_back1
Nback <- Nback %>%
  mutate(Spreadsheet = ifelse(Spreadsheet == "nback1", "n_back1", Spreadsheet),
         Spreadsheet = ifelse(Spreadsheet == "nback2", "n_back2", Spreadsheet))

#verify value for colum spreadsheet
Nback %>%
  group_by(Spreadsheet) %>%
  summarise(n = n())



Nback <- left_join(Nback, nb_target, by = c( "nback", "Spreadsheet", "condition"))

Nback %>%
  filter(is.na(notarget)) %>%
  group_by(Zone_Type) %>%
  summarise(n = n())#only zone.type likert, continue button...


```


#Block number index
```{r}
#add a column "block number" identifying and indexing each trial of session 1
#keep only first Run 1 for participants that did 2 or 3 Run1 for some reasons

Nback <- Nback %>%
  mutate(end = `Zone_Type` == "response_rating_scale_likert",
         block_nb = cumsum(end),
         block_nb = ifelse(end == T, block_nb - 1, block_nb)) %>%
  select(-end) #17482
```


#Confinement Tracker : remove runs completed when participants were not confined
```{r}

ConfTrack <- read.csv2("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/data control session/data-ConfinementTrack_All_Countries_2021-11-29.csv", sep = ",") #24716 obs


#verify number of participant for each country who answered to the Confinement Track questionnaire #24716 obs
ConfTrack %>%
  filter(`Question_Key` %in% 'ConfStage') %>%
    group_by(`Country`, PID) %>%
    summarise(PID = first(PID)) %>%
  group_by(`Country`) %>%
  summarise(n = n())

ConfTrack %>% distinct




# Translation "during"
during <- c("during", "pendant", "κατά τη διάρκεια", "sırasında", "durante", "規制（自粛）中", "Durante", "durante")

tracker <- ConfTrack %>% #3375
  filter(`Question_Key` %in% 'ConfStage' | `Question_Key` %in% 'ConfStage-2') %>%
  mutate(Conftrack = Response) %>%
  select(PID, Run, Conftrack, X)


# Remove duplicates, keep first row for each duplicate
first_track <- tracker %>%   #3278 obs
  group_by(PID) %>%
  group_by(Run, .add =T) %>%
  group_by(Conftrack, .add =T) %>%
  dplyr::slice(which.min(X)) %>%
  select(X)

tracks_to_rm <- setdiff(tracker$X, first_track$X)  #543 trials

tracker <- tracker %>%
        filter(!(X %in% tracks_to_rm)) %>%
        select(-X)
tracker #2041 PID during confinement for Run 1, 711 for run 2, and 526 for Run 3 : 3375 runs overall



#Verify number of occurrence for each Conftracker response
tracker %>%
  group_by(Conftrack, Run) %>%
  summarise(n = n())

#merge Conftracker and Nback, keep only responses for run made during confinement.
Nback <- left_join(Nback, tracker, by = c("PID", "Run")) %>%  
filter(Conftrack %in% during) #1.560.592 --> 1.525.289 obs



#verify number of participant for each Country: # AR:125/CA:33/FR:384/GR:131/IN:53/IT:151/JP:104/TR:146   #old : AR:142/CA:33/FR:383/GR:125/IN:53/IT:150/JP:104/TR:146
Nback %>%
  group_by(PID) %>%
  summarise(Country = first(Country)) %>%
  group_by(Country) %>%
  summarise(numbersubjects = n())


Nback %>%
  count(PID)   #1169 --> 1127 PID  (old --> 1136)
```


  
#Deal with attempts
```{r}

##keyboard response Attempts
Table_Attempts <- Nback %>%  
  group_by(Attempt) %>%
  count() %>%
  filter(as.numeric(Attempt) <10)
Table_Attempts
# 625 000 first attempts, ~70 000 more than one attempt

Nback <- Nback %>%            #removing all responses with more than one Attempt
  filter( Zone_Type != "response_keyboard" | Attempt %in% c("1", NA))
#~1.525.289  obs --> 1.416.280 obs


##Response text entry and likert Attempts

Nback %>% 
  filter(Zone_Type == "response_text_entry") %>%
  group_by(Attempt) %>%
  count()
# 100 responses with more than 1 Attempts

Nback %>% 
  filter(Zone_Type == "response_text_entry" & Attempt > 1) 
  
#... but some second attempts were not indexed (NA)

blockw2answers <- Nback %>%   #160 blocks among  with more than one attempts
  filter(Zone_Type == "response_text_entry") %>%
  group_by(block_nb, .add =T) %>%
   summarise(n = n()) %>%
  filter(n > 1)

Nback %>% 
  filter(block_nb %in% blockw2answers$block_nb)
  
  

#Responses for both attempts are identical --> keep the first one
  
  
  Nback <- Nback %>%            #removing all responses with more than one Attempt
  filter( Zone_Type != "response_text_entry" | Attempt %in% c("1", NA))
  
  
first_attempts <- Nback %>%   #16798 tasks
  filter(Zone_Type == "response_text_entry") %>%
  group_by(block_nb, .add =T) %>%
  dplyr::slice(which.min(timestamp)) %>%
    select(timestamp)

response_to_rm <- setdiff((filter(Nback, Zone_Type == "response_text_entry"))$timestamp, first_attempts$timestamp)  #144 trials

Nback %>% 
  filter(timestamp %in% response_to_rm)

Nback <- Nback %>%
        filter(!(timestamp %in% response_to_rm & Zone_Type == "response_text_entry"))  

# #Remove mysterious second 479 response_entry line :
# Nback <- Nback %>%
#   filter(!(block_nb == 479 & Zone_Type == "response_text_entry" & is.na(Response)))


# 1.416.280  - 156 --> 1.416.124 obs

# Remove duplicate responses
  Nback <- Nback %>% distinct #1402693



  Nback %>%
  count(PID)   #1127 PID
```



#Number of responses  --> few trials with too much responses, but lots of trials with few responses
```{r}
Nback %>%
  group_by(block_nb, .add = T) %>%
  filter(Response %in% c("Left", "Down")) %>%
  summarise(n= n(), duration = first(duration)) %>%
  group_by(duration, n) %>%
  summarise(nn = n())

number_resp = Nback %>%
   group_by(duration, ILI, nback) %>%
  group_by(block_nb, .add = T) %>%
  filter(Response %in% c("Left", "Down")) %>%
  summarise(n = n(), duration = first(duration))

number_resp %>%
  filter(!is.na(duration)) %>%
  ggplot(aes(n, group = duration, colour = duration)) +
  facet_grid(duration~ILI) +
           geom_histogram(position = "dodge", binwidth=1,
                   colour = "black" , fill="white") +
  labs(y = "count", x = "Number of Responses", title ="Number of responses (Down or Left) for 45s and 90s tasks") +
  theme(plot.title = element_text(face = "bold", "hjust" = 0.5)) 
  # scale_x_continuous(limits = c(0, 70), breaks = seq(from = 0, to = 70, by = 5))

#some task have 2 times or 3 times more responses than expected


nbresp <- Nback %>%
  filter(Zone_Type == "response_keyboard") %>%
  select(block_nb, condition) %>%
    group_by(block_nb) %>%
summarise(n = n(),condition = first(condition))
nbresp

blocktorm15 <- nbresp %>%
  filter(condition == 5 | condition == 1) %>%
  filter(is.na(block_nb) | n > 65)

blocktorm26 <- nbresp %>%
  filter(condition == 6 | condition == 2) %>%
  filter(is.na(block_nb) | n>125)

blocktorm37 <- nbresp %>%
  filter(condition == 3 | condition == 7) %>%
  filter(is.na(block_nb) | n > 57)

blocktorm48 <- nbresp %>%
  filter(condition == 4 | condition == 8) %>%
  filter(is.na(block_nb) | n>107)

blocktorm <- bind_rows(blocktorm15, blocktorm26, blocktorm37, blocktorm48) #6 blocks


Nback <- Nback %>%
  filter(!(block_nb %in% blocktorm$block_nb))

blocknb <-Nback %>%
  group_by(block_nb) %>%
  summarise(n = n()) 
blocknb
#17132 trials

Nback %>%
  count(PID)   #1127--> 1126 PID
```


##Filtering and formatting data
```{r}

# Remove excess tasks (for duration estimation and likert response columns)
first_trial <- Nback %>%   #16163 tasks
  filter(Zone_Type == "response_text_entry") %>%
  group_by(PID) %>%
  group_by(nback, .add =T) %>%
  group_by(Run, .add =T) %>%
  group_by(randomblock, .add =T) %>%
  dplyr::slice(which.min(timestamp)) %>%
  select(block_nb)


trials_to_rm <- setdiff(blocknb$block_nb, first_trial$block_nb)  #969 trials

Nback <- Nback %>%
        filter(!(block_nb %in% trials_to_rm))  # --> 1,345,551 observations



#merge entry response (likert scale and duration estimation) with other responses
df1 <- Nback %>%  #594.473
  filter(Zone_Type == "response_keyboard") %>%
  select(-Zone_Type) 

df2 <-Nback %>% #16261 obs
  filter(Zone_Type == "response_text_entry" | Zone_Type == "response_rating_scale_likert") %>% 
  select(-timestamp, -UTC_Date, - Reaction_Time, - Attempt, - ntarget, -notarget) %>% 
  mutate(Response = as.character(Response)) %>% 
  pivot_wider(values_from = Response, names_from = Zone_Type) 


Nback_all <- left_join(df1, df2, by = c("PID", "Country", "Session", "nback", "randomblock", "Run", "block_nb",  "Conftrack", "Age", "Sex", "Handedness")) %>%
  select(-Correct.y, - duration.y, -ILI.y, - condition.y, -Spreadsheet.y, -Experiment_ID.y) %>%
  filter(!is.na(response_text_entry)) %>%
  rename(duration = duration.x, 
         ILI = ILI.x, 
         Correct = Correct.x,
         condition = condition.x,
         response_arrow = Response,
         likert = response_rating_scale_likert) #593.551 obs

#number of subjects
Nback_all %>%  
  group_by(PID) %>%
  summarise(n = n())

#number of blocks
blocks <- Nback %>% 
  group_by(block_nb) %>%
  summarise(n = n())
blocks 

blockswithnoresps <- setdiff(first_trial$block_nb, blocks$block_nb)
# 1118 participants + 16163 blocks   (blocks with no responses (~0))
#participants who did not make an estimate have been removed


```



##translating subjective duration estimates and add them to Nback
```{r}
##formatting and translating subjective duration estimate

#getting rid of spaces, punctuation, "::" and words that beginning by m, d or λ (for minutes, dakika and λεπτό))     
subj_duration = Nback %>% filter(Zone_Type == "response_text_entry") %>% 
  select(PID, Response, Run, randomblock, nback) %>% 
  mutate(Response = str_to_lower(as.character(Response)),
          Response = str_replace_all(Response, "０", "0"),
          Response = str_replace_all(Response, "１", "1"),
          Response = str_replace_all(Response, "２", "2"),
          Response = str_replace_all(Response, "３", "3"),
          Response = str_replace_all(Response, "deytera", ""),
          Response = str_replace_all(Response, regex("m([a-z]{0,})"), ":"),
          Response = str_replace_all(Response, regex("d([a-z]{0,})"), ":"),
          Response = str_replace_all(Response, regex("λ([a-z]{0,})"), ":"),
          Response = str_replace_all(Response, regex("[:punct:]"), ":"),
          Response = str_replace_all(Response, regex("[:space:]"), ""),
          Response = str_replace_all(Response, "::", ":"))
#16266 obs --> create some duplicates

subj_duration1 <- subj_duration %>% 
  mutate(translated = strtoi(
    as.difftime(Response, format = "00:%M:%S", units = "sec")))  #recognize duration with format (mm:ss)

subj_duration2 <- subj_duration1 %>% 
  filter(is.na(translated)) %>%   #among duration not yet translated
  mutate(translated = strtoi(
    as.difftime(Response, format = "%M:%S", units = "sec")))  #recognize duration with format (mm:ss)

subj_duration3 <- subj_duration2 %>% 
  filter(is.na(translated)) %>%   #among duration not yet translated
  mutate(translated = ifelse(
    str_detect(Response, regex("(\\d{1,2})s([a-z]{0,})")),     #if two digit only, recognize them as (ss)
     strtoi(as.difftime(Response, format = c("%S"), units = "sec")),
    NA)) 

subj_duration4 <- subj_duration3 %>% 
  filter(is.na(translated)) %>%   #among duration not yet translated
  mutate( translated = ifelse(
    str_detect(Response, regex("\\d{4}")),    #if series of 4 digits, recognize them as (mmss)
     strtoi(as.difftime(Response, format = c("%M%S"), units = "sec")),
    NA)) 

subj_duration5 = subj_duration4 %>% 
  filter(is.na(translated)) %>%  #among duration not yet translated
  mutate(translated = parse_number(Response)) #extract only the number from the string


#merge each dataframe with their translated values
subj_duration = full_join(subj_duration1, subj_duration2, copy = F) %>% filter(!is.na(translated))
subj_duration = full_join(subj_duration, subj_duration3, copy = F) %>% filter(!is.na(translated))
subj_duration = full_join(subj_duration, subj_duration4, copy = F) %>% filter(!is.na(translated))
subj_duration = full_join(subj_duration, subj_duration5, copy = F) %>% 
# subj_duration = full_join(subj_duration, subj_duration6, copy = F) %>%
  mutate(translated = ifelse(translated > 500, translated/60, translated),
         translated = ifelse(translated < 6, translated*60, translated))  

#get the duration estimation that ave strange formats and translate them manually 
no_translated = subj_duration  %>% 
  filter(is.na(translated))  
  # write_csv(file.path(outdir,'no_translated.csv'))     #73 no translated 


#duplicates are removed 16266 --> 16270 obs
subj_duration <- subj_duration %>% distinct() 

#merge subj_duration with Nback ==> add translated estimates
Nback_all <- left_join(Nback_all, subj_duration, by = c("PID", "Run", "nback", "randomblock")) %>%
  filter(!is.na(translated))  
Nback_all #592.202 obs

#Count overall number of trials (8 trials per Run per Participants ) 

Nback_blocks <- Nback_all %>%
  group_by(PID) %>%
  group_by(nback, .add = T) %>%
  group_by(Run, .add = T) %>%
  group_by(duration, ILI, .add = T) %>%
  summarise( translated = first(translated),
             likert = first(likert),
            block_nb = first(block_nb)) %>%
  ungroup()

Nback_blocks #15.839 trials



Nback_all <- Nback_all %>% select(Session, Country, PID, Run, block_nb, nback, ILI, likert, duration, condition, translated, Reaction_Time, response_arrow, ntarget, notarget, Correct, Age, Sex, Handedness, timestamp) %>%
write_csv(file.path("/home/cyril/Documents/Cognition & Brain Dynamics/TimeSocialDistancing/nback_TSD",'S1global_nback.csv'))


#1100 participants
 Nback_all %>%
  count(PID) 
```

